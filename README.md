# Credit Default Risk Scoring

## Постановка задачи

Мой интерес к этой теме связан с тем, что недавно я подробно разбирался в причинах кризиса 2008 года и увидел, насколько важно корректно оценивать кредитный риск. Цель проекта – построить модель кредитного скоринга, которая по заявке клиента оценивает риск дефолта и выдаёт вероятность невозврата. И хотя тот мировой кризис был связан прежде всего с ипотечными кредитами, сам подход к управлению риском универсален: решения должны опираться на данные и количественные критерии, а качество модели – быть воспроизводимо проверяемым.

### Формат входных и выходных данных

**Входные данные:**

- Заявка на кредит, представленная в виде вектора признаков после препроцессинга.
- Формат: tensor<float>[1, D], где D – число признаков после преобразований.

**Выходные данные:**

- Вероятность дефолта для этой заявки (число от 0 до 1).
- Формат: tensor<float>[1, 1].
  Порог для получения класса из вероятности не задан и будет выбираться на валидационной выборке в зависимости от целевой метрики.

### Метрики

- **ROC-AUC** – основная метрика. Она показывает, насколько хорошо модель ранжирует заявки по уровню риска и отделяет более рискованные заявки от менее рискованных, не привязываясь к фиксированному порогу.
  Ожидаем получить значение заметно выше уровня случайного угадывания 0.5. Для базовой модели на табличных данных реалистичный ориентир – около 0.70–0.75. Точный целевой уровень уточним после первого бейзлайн-запуска.
- **LogLoss** – вспомогательная метрика. Она оценивает качество вероятностных предсказаний и сильнее штрафует уверенные ошибки, поэтому подходит для задачи, где на выходе требуется вероятность дефолта.
  Ожидаем значение ниже, чем у наивного бейзлайна “предсказывать одну и ту же вероятность для всех заявок” (т.е. константа, равная среднему TARGET на train). Конкретное целевое значение также уточним после вычисления этого бейзлайна.

### Валидация и тест

Используем стратифицированное разбиение исходного датасета, чтобы сохранить долю классов во всех выборках. Для воспроизводимости результатов зафиксируем random_seed, а также будем использовать одинаковое разбиение при каждом запуске. Данные разделим на train/val/test в пропорции 70/15/15.

### Датасет

- **Источник**: Kaggle, соревнование Home Credit Default Risk.
- **Состав данных**: основной датасет содержит главную таблицу заявок (train и test). Также есть дополнительные таблицы, но в базовой версии проекта будем использовать только основную таблицу заявок.
- **Количество сэмплов**: train – 307 511 заявок, test – 48 744 заявок.
- **Количество признаков**: 121 (без TARGET).
- **Объём**: около 180 МБ (train + test).
- **Возможные проблемы**: много категориальных признаков и пропусков (нужен аккуратный препроцессинг), возможен дисбаланс классов.

## Моделирование

### Бейзлайн

В качестве baseline решения будет реализована минимальная нейросеть для табличных данных – однослойный перцептрон.

**Архитектура**: один линейный слой (Linear) и выход на одно значение (вероятность дефолта). Это позволит максимально быстро получить стартовые метрики, с которыми будет сравниваться основная модель.

### Основная модель

Будет использована простая MLP для табличных данных на PyTorch Lightning.

**Архитектура**: два скрытых слоя (Linear + ReLU) и выходной слой на одно значение (вероятность дефолта). Для снижения переобучения будет добавлена базовая регуляризация (dropout или weight decay). Обучение будет проводиться с ранней остановкой по валидации.

## Внедрение

Модель будет оформлена в виде Python-пакета, который можно установить и использовать как библиотеку в других проектах. Пакет будет предоставлять API для загрузки сохранённых артефактов (веса модели и параметры препроцессинга) и получения предсказания по одной заявке:

- функция **predict_proba(features)** будет возвращать вероятность дефолта,
- функция **predict(features, threshold)** будет возвращать класс по порогу.
